# simulate_and_recover

This project was about testing whether the EZ diffusion model could recover its own parameters. The idea was to generate data using the model, then try to estimate the original parameters from that data. If the model is consistent, the recovered values should be close to the true ones. I ran 3,000 simulations with different sample sizes (N = 10, 40, and 4000) and checked the bias and squared error. Bias should average to zero, and squared error should decrease as N increases, meaning larger datasets produce more precise estimates.

At first, setting up the simulation seemed straightforward. I wrote a Python script to generate random parameters, compute summary statistics using the forward equations, and estimate the parameters back using the inverse equations. But soon, I ran into a ZeroDivisionError when computing the log of accuracy. This happened when accuracy was exactly 1 or 0, making the denominator in log(R_obs / (1 - R_obs)) equal to zero. This completely broke the estimation process, causing NaN values. AI helped here by pointing out that I could prevent division by zero by adding a small offset (epsilon = 1e-6). After making that adjustment, the script ran without crashing, and I could move on.

Once the simulation was working, I needed to save results to CSV files. I initially included the sample size (N) as a column but later realized it wasn’t necessary. I updated the script to remove it, but when I tried committing the changes, Git wouldn’t track the modified files. AI suggested running git status to check what was happening. It turned out Git saw the changes but wasn’t staging them. I used git add results_N10.csv results_N40.csv results_N4000.csv, and that solved the issue.

Pushing the files to GitHub also gave me trouble. First, I got an error saying "src refspec main does not match any." AI explained that this happens when the main branch isn’t set up correctly. It suggested running git branch -M main to rename the branch and git push -u origin main to establish the connection with GitHub. That fixed the problem, and I could push my code. But then I ran into another issue: "Updates were rejected because the remote contains work that you do not have locally." This happened because my GitHub repo already had changes I hadn’t pulled. AI walked me through using git pull origin main --rebase to sync my local repo before pushing. 

Beyond debugging, AI helped optimize the code. For example, formatting the CSV output using fmt="%.5f" to ensure consistent decimal precision. These small optimizations made the script cleaner and easier to read.

This project reinforced the importance of debugging and testing. AI played a big role in solving problems quickly, whether it was fixing mathematical errors, handling Git issues, or improving efficiency. It made the process much smoother. In the end, the results matched expectations. Bias averaged close to zero, meaning the model wasn’t systematically under- or over-estimating parameters. Squared error decreased as N increased, showing that larger datasets improved precision.